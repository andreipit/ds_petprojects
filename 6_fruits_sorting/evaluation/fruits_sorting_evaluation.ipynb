{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "    \n",
    "\n",
    "\n",
    "# read\n",
    "fruits = pd.read_table('fruit_data_my.txt')\n",
    "lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique())) #{1: 'apple', 2: 'mandarin', 3: 'orange', 4: 'lemon'}\n",
    "\n",
    "# split\n",
    "X = fruits[['mass', 'width', 'height', 'color_score']]\n",
    "y = fruits['fruit_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)              # default is 75% / 25% train-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 1.00 precision: 1.00 accuracy: 1.00\n",
      "roc_auc: 1.00\n",
      "recall: 1.00 precision: 0.70 accuracy: 0.70\n",
      "roc_auc: 0.50\n",
      "recall: 0.57 precision: 1.00 accuracy: 0.70\n",
      "roc_auc: 0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# recall = percent of judged criminals from all criminals\n",
    "# precision = percent of criminals from all judged\n",
    "def print_scores(y_test_my, y_pred_my):\n",
    "    print('recall: {:.2f}'.format(recall_score(y_test_my, y_pred_my)), 'precision: {:.2f}'.format(precision_score(y_test_my, y_pred_my)), 'accuracy: {:.2f}'.format(accuracy_score(y_test_my, y_pred_my))  )\n",
    "    print('roc_auc: {:.2f}'.format(roc_auc_score(y_test_my, y_pred_my)))\n",
    "\n",
    "y_test_my = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1] # 3 criminals / 7 good\n",
    "y_pred_my = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1] # perfect! r=1, p=1, a=1\n",
    "print_scores(y_test_my, y_pred_my)\n",
    "y_test_my = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1] # 3 criminals / 7 good\n",
    "y_pred_my = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] # tyranny: r=1, p=0.7, a=0.7 (all criminals judged, but good men too)\n",
    "print_scores(y_test_my, y_pred_my)\n",
    "y_test_my = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1] # 3 criminals / 7 good\n",
    "y_pred_my = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1] # democracy: r=0.57, p=1, a=0.7 (no one good man is judged)\n",
    "print_scores(y_test_my, y_pred_my)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.00 precision: 0.00 accuracy: 0.20\n",
      "recall: 0.00 precision: 0.00 accuracy: 0.10\n",
      "recall: 0.00 precision: 0.00 accuracy: 0.00\n",
      "recall: 0.14 precision: 0.25 accuracy: 0.10\n",
      "recall: 0.29 precision: 0.40 accuracy: 0.20\n",
      "recall: 0.43 precision: 0.50 accuracy: 0.30\n",
      "recall: 0.57 precision: 0.57 accuracy: 0.40\n",
      "recall: 0.71 precision: 0.62 accuracy: 0.50\n",
      "recall: 0.86 precision: 0.67 accuracy: 0.60\n",
      "recall: 1.00 precision: 0.70 accuracy: 0.70\n",
      "back way\n",
      "recall: 0.14 precision: 1.00 accuracy: 0.40\n",
      "recall: 0.29 precision: 1.00 accuracy: 0.50\n",
      "recall: 0.43 precision: 1.00 accuracy: 0.60\n",
      "recall: 0.57 precision: 1.00 accuracy: 0.70\n",
      "recall: 0.71 precision: 1.00 accuracy: 0.80\n",
      "recall: 0.86 precision: 1.00 accuracy: 0.90\n",
      "recall: 1.00 precision: 1.00 accuracy: 1.00\n",
      "recall: 1.00 precision: 0.88 accuracy: 0.90\n",
      "recall: 1.00 precision: 0.78 accuracy: 0.80\n",
      "recall: 1.00 precision: 0.70 accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "def print_scores(y_test_my, y_pred_my):\n",
    "    print('recall: {:.2f}'.format(recall_score(y_test_my, y_pred_my)), 'precision: {:.2f}'.format(precision_score(y_test_my, y_pred_my)), 'accuracy: {:.2f}'.format(accuracy_score(y_test_my, y_pred_my))  )\n",
    "# print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]) #recall: 0.57 precision: 1.00 accuracy: 0.70\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) #\n",
    "print('back way')\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1]) #\n",
    "print_scores([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20889aeb0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterate throug all possible predictions, that give accuracy = 0.7 (0.7, cause in our y_score a=0.7)\n",
    "# draws for each those y_prediction - prec and recall\n",
    "# accuracy - is default (one of many possible) thresholds\n",
    "\n",
    "\n",
    "def draw_precision_recall(y_test, y_score):\n",
    "    from sklearn.metrics import average_precision_score\n",
    "#     average_precision = average_precision_score(y_test, y_score)\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "#     plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "    plt.show()\n",
    "    \n",
    "y_test = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1] # 3 criminals / 7 good\n",
    "y_score =[0, 0, 0, 0, 0, 0, 1, 1, 1, 1] # democracy: r=0.57, p=1, a=0.7 (no one good man is judged)\n",
    "draw_precision_recall(y_test, y_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-28-8a5d5e8e32e2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-28-8a5d5e8e32e2>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    from sklearn.metrics import precision_recall_curve\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    lr = LogisticRegression().fit(X_train, y_train)\n",
    "    lr_predicted = lr.predict(X_test)\n",
    "    def plot_precision_recall():\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, lr_predicted)\n",
    "        closest_zero = np.argmin(np.abs(thresholds))\n",
    "        closest_zero_p = precision[closest_zero]\n",
    "        closest_zero_r = recall[closest_zero]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.xlim([0.0, 1.01])\n",
    "        plt.ylim([0.0, 1.01])\n",
    "        plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "        plt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\n",
    "        plt.xlabel('Precision', fontsize=16)\n",
    "        plt.ylabel('Recall', fontsize=16)\n",
    "        plt.axes().set_aspect('equal')\n",
    "        plt.show()\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 3, 4, 4, 2, 1, 2, 3, 4, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "y_test_predicted\n",
    "\n",
    "# print('knn test score: {:.3f}'.format(knn.score(X_test, y_test)))\n",
    "# print('knn predicts lemon as: {:}'.format(lookup_fruit_name[np.round(knn.predict([[208, 6.7, 11.2, 48]]),0)[0]] ))\n",
    "# recall = recall_score(y_test, y_train)\n",
    "# precision = precision_score(y_test, svm_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn test score: 0.933\n",
      "knn predicts lemon as: lemon\n",
      "knnreg test score: 0.513\n",
      "knnreg predicts lemon as: lemon\n",
      "linreg test score: 0.541\n",
      "linreg predicts lemon as: lemon\n",
      "linridge score (test): 0.445\n",
      "linridge predicts lemon as: lemon\n",
      "linridge alpha= 0 score (test): 0.541\n",
      "linridge alpha= 1 score (test): 0.536\n",
      "linridge alpha= 10 score (test): 0.488\n",
      "linridge alpha= 20 score (test): 0.445\n",
      "linridge alpha= 50 score (test): 0.362\n",
      "linridge alpha= 100 score (test): 0.292\n",
      "linridge alpha= 1000 score (test): 0.160\n",
      "linlasso score (test): 0.230\n",
      "linlasso predicts lemon as: orange\n",
      "linlasso alpha= 0.5 score (test): 0.230\n",
      "linlasso alpha= 1 score (test): 0.126\n",
      "linlasso alpha= 2 score (test): 0.119\n",
      "linlasso alpha= 3 score (test): 0.113\n",
      "linlasso alpha= 5 score (test): 0.099\n",
      "linlasso alpha= 10 score (test): 0.050\n",
      "linlasso alpha= 20 score (test): -0.017\n",
      "linlasso alpha= 50 score (test): -0.017\n",
      "poly degree  1 score (test): 0.541\n",
      "poly degree  1 predicts:  [ 2.07003408  2.03690971  1.69079894  2.15418561  3.21021501  4.17496149\n",
      "  2.04843999  2.00936375  1.93563596  2.64944001  3.21126851  2.44100067\n",
      "  1.96557798  2.27035347  2.40750246]\n",
      "poly degree  3 score (test): -9.582\n",
      "poly degree  3 predicts:  [ 11.39639664   1.85924787   5.57551728   3.19491502   5.87025984\n",
      "   5.59029178   2.59900294   4.02575429   2.01260604   5.19096611\n",
      "   6.30126844  -2.04860969   1.80708571  -1.36262279   1.83788577]\n",
      "poly degree  6 score (test): -27.214\n",
      "poly degree  6 predicts:  [ 21.72538432   1.8170717    0.16087635   3.8525472    4.77042518\n",
      "   4.38662217   2.05236083   4.27811736   2.21068504   8.89451703\n",
      "   4.97541428  -1.61499439   2.0380024    5.04375895   2.08320692]\n",
      "poly degree  9 score (test): -107.318\n",
      "poly degree  9 predicts:  [ 43.15901057   1.82267141   3.15814149   3.8908055    3.35883814\n",
      "   5.17503154   1.9754047    4.39534549   2.01875396   9.59293895\n",
      "   4.0825661    0.35741078   1.9426462    3.20877966   1.95710275]\n"
     ]
    }
   ],
   "source": [
    "def scale_0_4(value):\n",
    "    value = 4. if value > 4 else value\n",
    "    value = 1. if value < 1 else value\n",
    "    return value\n",
    "\n",
    "# scale\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# plot\n",
    "# from library import adspy_shared_utilities as asu\n",
    "# asu.plot_fruit_knn(X_train, y_train, 5, 'uniform')   \n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train)\n",
    "print('knn test score: {:.3f}'.format(knn.score(X_test, y_test)))\n",
    "print('knn predicts lemon as: {:}'.format(lookup_fruit_name[np.round(knn.predict([[208, 6.7, 11.2, 48]]),0)[0]] ))\n",
    "\n",
    "# KNN regression\n",
    "knnreg = KNeighborsRegressor(n_neighbors = 7).fit(X_train, y_train)\n",
    "print('knnreg test score: {:.3f}'.format(knnreg.score(X_test, y_test)))\n",
    "print('knnreg predicts lemon as: {:}'.format(lookup_fruit_name[np.round(knnreg.predict([[208, 6.7, 11.2, 48]]),0)[0]] ))\n",
    "\n",
    "# Linear regression\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "print('linreg test score: {:.3f}'.format(linreg.score(X_test, y_test)))\n",
    "print('linreg predicts lemon as: {:}'.format(lookup_fruit_name[np.round(scale_0_4(linreg.predict([[208, 6.7, 11.2, 48]])),0)] ))\n",
    "\n",
    "# Ridge regression\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "print('linridge score (test): {:.3f}'.format(linridge.score(X_test, y_test)))\n",
    "print('linridge predicts lemon as: {:}'.format(lookup_fruit_name[np.round(scale_0_4(linridge.predict([[208, 6.7, 11.2, 48]])),0)] ))\n",
    "# Ridge regression with regularization parameter: alpha\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train, y_train)\n",
    "    print('linridge alpha=',this_alpha,'score (test): {:.3f}'.format(linridge.score(X_test, y_test)))\n",
    "    \n",
    "# Lasso regression\n",
    "linlasso = Lasso(alpha=0.5, max_iter = 10000).fit(X_train, y_train)\n",
    "print('linlasso score (test): {:.3f}'.format(linlasso.score(X_test, y_test)))\n",
    "print('linlasso predicts lemon as: {:}'.format(lookup_fruit_name[np.round(scale_0_4(linlasso.predict([[208, 6.7, 11.2, 48]])),0)[0]] ))\n",
    "# Lasso regression with regularization parameter: alpha        \n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train, y_train)\n",
    "    print('linlasso alpha=',alpha,'score (test): {:.3f}'.format(linlasso.score(X_test, y_test)))\n",
    "\n",
    "    \n",
    "    \n",
    "# Polynomial regression\n",
    "# X_poly = PolynomialFeatures(degree=2).fit_transform(X)\n",
    "# X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_poly, y, random_state=0)\n",
    "# linreg = LinearRegression().fit(X_train_p, y_train_p)\n",
    "# print('poly + linreg test score: {:.3f}'.format(linreg.score(X_test_p, y_test_p)))\n",
    "# linreg.predict([[208, 6.7, 11.2, 48]])\n",
    "# print('poly + linreg predicts lemon as: {:}'.format(lookup_fruit_name[np.round(scale_0_4(linreg.predict([[208, 6.7, 11.2, 48]])),0)] ))\n",
    "\n",
    "\n",
    "def add_features(line, deg): # for prediction we should convert test values too\n",
    "#     col = line.reshape(len(line),1) # transposes, line of 11 -> to column of 11\n",
    "    col = line\n",
    "    poly = PolynomialFeatures(degree=deg)\n",
    "    value = poly.fit_transform(col)\n",
    "    return value\n",
    "for i, value in enumerate([1,3,6,9]):\n",
    "    # prepare data\n",
    "    X_train_poly = add_features(X_train, value)              # 1row x 11cols  ->  11rows x 7cols (11,7)\n",
    "#     X_axis_poly = add_features(np.linspace(0,10,100), value) # 1row x 100cols -> 100rows x 7cols (100,7)\n",
    "    X_test_poly = add_features(X_test, value)                # 1row x  4cols  ->   4rows x 7cols (4,7)\n",
    "\n",
    "    # learn\n",
    "    linreg = LinearRegression().fit(X_train_poly, y_train)\n",
    "\n",
    "    # score\n",
    "    print('poly degree ', value, 'score (test): {:.3f}'.format(linreg.score(X_test_poly, y_test)))\n",
    "\n",
    "    # use\n",
    "    #y_train_predict_one = linreg.predict([list(X_train_poly[0,:])]) #input=> 1rows x 7cols, output=> 1.22595734, but y_train[0] = 1.21213026\n",
    "    #y_test_predict_one = linreg.predict([list(X_test_poly[0,:])])   #input=> 1rows x 7cols, output=> 0.98597635, but y_test[0] = 0.99517935\n",
    "    y_test_predict = linreg.predict(X_test_poly)   #input=> 4rows x 7cols, output=> 1row x 4cols: array([ 0.98597635, -0.18539455,  0.37296501,  1.18016858]), but y_test = [ 0.99517935, -0.16081   ,  0.3187423 ,  1.53763897]\n",
    "    #y_axis_predict = linreg.predict(X_axis_poly)   #input=> 100rows x 7cols, output=> 1row x 100cols\n",
    "    print('poly degree ', value, 'predicts: ',y_test_predict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next in Module 2:\n",
    "\n",
    "# Linear models for classification\n",
    "#     Logistic regression\n",
    "#     Logistic regression for binary classification on fruits dataset using height, width features (positive class: apple, negative class: others)\n",
    "#     Logistic regression on simple synthetic dataset\n",
    "#     Logistic regression regularization: C parameter\n",
    "#     Application to real dataset\n",
    "\n",
    "# Support Vector Machines\n",
    "#     Linear Support Vector Machine\n",
    "#     Linear Support Vector Machine: C parameter\n",
    "#     Application to real dataset\n",
    "    \n",
    "# Multi-class classification with linear models\n",
    "#     LinearSVC with M classes generates M one vs rest classifiers.\n",
    "#     Multi-class results on the fruit dataset\n",
    "    \n",
    "# Kernelized Support Vector Machines\n",
    "#     Classification\n",
    "#     Support Vector Machine with RBF kernel: gamma parameter\n",
    "#     Support Vector Machine with RBF kernel: using both C and gamma parameter\n",
    "#     Application of SVMs to a real dataset: unnormalized data\n",
    "#     Application of SVMs to a real dataset: normalized data with feature preprocessing using minmax scaling\n",
    "        \n",
    "# Cross-validation\n",
    "#     Example based on k-NN classifier with fruit dataset (2 features)\n",
    "#     A note on performing cross-validation for more advanced scenarios.\n",
    "#     Validation curve example\n",
    "\n",
    "# Decision Trees\n",
    "#     Setting max decision tree depth to help avoid overfitting\n",
    "#     Visualizing decision trees\n",
    "#     Pre-pruned version (max_depth = 3)\n",
    "#     Feature importance\n",
    "#     Decision Trees on a real-world dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933333333333\n",
      "2.45419746756\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "fruits = pd.read_table('fruit_data_my.txt')\n",
    "X = fruits[['mass', 'width', 'height', 'color_score']]\n",
    "y = fruits['fruit_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)              # default is 75% / 25% train-test split\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)                                            # Create classifier object\n",
    "knn.fit(X_train, y_train)                                                              # Train the classifier\n",
    "print(knn.score(X_test, y_test))\n",
    "print(log_loss(y_test, knn.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
