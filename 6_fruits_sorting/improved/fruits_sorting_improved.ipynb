{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn test score: 0.933\n",
      "knn predicts lemon as: lemon\n",
      "knn test score: 0.933\n",
      "knn predicts lemon as: lemon\n",
      "knnreg test score: 0.513\n",
      "knnreg predicts lemon as: lemon\n",
      "linreg test score: 0.541\n",
      "linreg predicts lemon as: lemon\n",
      "linridge score (test): 0.445\n",
      "linridge predicts lemon as: lemon\n",
      "linridge alpha= 0 score (test): 0.541\n",
      "linridge alpha= 1 score (test): 0.536\n",
      "linridge alpha= 10 score (test): 0.488\n",
      "linridge alpha= 20 score (test): 0.445\n",
      "linridge alpha= 50 score (test): 0.362\n",
      "linridge alpha= 100 score (test): 0.292\n",
      "linridge alpha= 1000 score (test): 0.160\n",
      "linlasso score (test): 0.230\n",
      "linlasso predicts lemon as: orange\n",
      "linlasso alpha= 0.5 score (test): 0.230\n",
      "linlasso alpha= 1 score (test): 0.126\n",
      "linlasso alpha= 2 score (test): 0.119\n",
      "linlasso alpha= 3 score (test): 0.113\n",
      "linlasso alpha= 5 score (test): 0.099\n",
      "linlasso alpha= 10 score (test): 0.050\n",
      "linlasso alpha= 20 score (test): -0.017\n",
      "linlasso alpha= 50 score (test): -0.017\n",
      "poly degree  1 score (test): 0.541\n",
      "poly degree  1 predicts:  [ 2.07003408  2.03690971  1.69079894  2.15418561  3.21021501  4.17496149\n",
      "  2.04843999  2.00936375  1.93563596  2.64944001  3.21126851  2.44100067\n",
      "  1.96557798  2.27035347  2.40750246]\n",
      "poly degree  3 score (test): -9.582\n",
      "poly degree  3 predicts:  [ 11.39639664   1.85924787   5.57551728   3.19491502   5.87025984\n",
      "   5.59029178   2.59900294   4.02575429   2.01260604   5.19096611\n",
      "   6.30126844  -2.04860969   1.80708571  -1.36262279   1.83788577]\n",
      "poly degree  6 score (test): -27.214\n",
      "poly degree  6 predicts:  [ 21.72538432   1.8170717    0.16087635   3.8525472    4.77042518\n",
      "   4.38662217   2.05236083   4.27811736   2.21068504   8.89451703\n",
      "   4.97541428  -1.61499439   2.0380024    5.04375895   2.08320692]\n",
      "poly degree  9 score (test): -107.318\n",
      "poly degree  9 predicts:  [ 43.15901057   1.82267141   3.15814149   3.8908055    3.35883814\n",
      "   5.17503154   1.9754047    4.39534549   2.01875396   9.59293895\n",
      "   4.0825661    0.35741078   1.9426462    3.20877966   1.95710275]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "    \n",
    "\n",
    "\n",
    "# read\n",
    "fruits = pd.read_table('fruit_data_my.txt')\n",
    "lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique())) #{1: 'apple', 2: 'mandarin', 3: 'orange', 4: 'lemon'}\n",
    "\n",
    "# split\n",
    "X = fruits[['mass', 'width', 'height', 'color_score']]\n",
    "y = fruits['fruit_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)              # default is 75% / 25% train-test split\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train)\n",
    "print('knn test score: {:.3f}'.format(knn.score(X_test, y_test)))\n",
    "print('knn predicts lemon as: {:}'.format(lookup_fruit_name[np.round(knn.predict([[208, 6.7, 11.2, 48]]),0)[0]] ))\n",
    "# recall = recall_score(y_test, y_train)\n",
    "# precision = precision_score(y_test, svm_predicted)\n",
    "\n",
    "def scale_0_4(value):\n",
    "    value = 4. if value > 4 else value\n",
    "    value = 1. if value < 1 else value\n",
    "    return value\n",
    "\n",
    "# scale\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# plot\n",
    "# from library import adspy_shared_utilities as asu\n",
    "# asu.plot_fruit_knn(X_train, y_train, 5, 'uniform')   \n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train)\n",
    "print('knn test score: {:.3f}'.format(knn.score(X_test, y_test)))\n",
    "print('knn predicts lemon as: {:}'.format(lookup_fruit_name[np.round(knn.predict([[208, 6.7, 11.2, 48]]),0)[0]] ))\n",
    "\n",
    "# KNN regression\n",
    "knnreg = KNeighborsRegressor(n_neighbors = 7).fit(X_train, y_train)\n",
    "print('knnreg test score: {:.3f}'.format(knnreg.score(X_test, y_test)))\n",
    "print('knnreg predicts lemon as: {:}'.format(lookup_fruit_name[np.round(knnreg.predict([[208, 6.7, 11.2, 48]]),0)[0]] ))\n",
    "\n",
    "# Linear regression\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "print('linreg test score: {:.3f}'.format(linreg.score(X_test, y_test)))\n",
    "print('linreg predicts lemon as: {:}'.format(lookup_fruit_name[np.round(scale_0_4(linreg.predict([[208, 6.7, 11.2, 48]])),0)] ))\n",
    "\n",
    "# Ridge regression\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "print('linridge score (test): {:.3f}'.format(linridge.score(X_test, y_test)))\n",
    "print('linridge predicts lemon as: {:}'.format(lookup_fruit_name[np.round(scale_0_4(linridge.predict([[208, 6.7, 11.2, 48]])),0)] ))\n",
    "# Ridge regression with regularization parameter: alpha\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train, y_train)\n",
    "    print('linridge alpha=',this_alpha,'score (test): {:.3f}'.format(linridge.score(X_test, y_test)))\n",
    "    \n",
    "# Lasso regression\n",
    "linlasso = Lasso(alpha=0.5, max_iter = 10000).fit(X_train, y_train)\n",
    "print('linlasso score (test): {:.3f}'.format(linlasso.score(X_test, y_test)))\n",
    "print('linlasso predicts lemon as: {:}'.format(lookup_fruit_name[np.round(scale_0_4(linlasso.predict([[208, 6.7, 11.2, 48]])),0)[0]] ))\n",
    "# Lasso regression with regularization parameter: alpha        \n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train, y_train)\n",
    "    print('linlasso alpha=',alpha,'score (test): {:.3f}'.format(linlasso.score(X_test, y_test)))\n",
    "\n",
    "    \n",
    "    \n",
    "# Polynomial regression\n",
    "# X_poly = PolynomialFeatures(degree=2).fit_transform(X)\n",
    "# X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_poly, y, random_state=0)\n",
    "# linreg = LinearRegression().fit(X_train_p, y_train_p)\n",
    "# print('poly + linreg test score: {:.3f}'.format(linreg.score(X_test_p, y_test_p)))\n",
    "# linreg.predict([[208, 6.7, 11.2, 48]])\n",
    "# print('poly + linreg predicts lemon as: {:}'.format(lookup_fruit_name[np.round(scale_0_4(linreg.predict([[208, 6.7, 11.2, 48]])),0)] ))\n",
    "\n",
    "\n",
    "def add_features(line, deg): # for prediction we should convert test values too\n",
    "#     col = line.reshape(len(line),1) # transposes, line of 11 -> to column of 11\n",
    "    col = line\n",
    "    poly = PolynomialFeatures(degree=deg)\n",
    "    value = poly.fit_transform(col)\n",
    "    return value\n",
    "for i, value in enumerate([1,3,6,9]):\n",
    "    # prepare data\n",
    "    X_train_poly = add_features(X_train, value)              # 1row x 11cols  ->  11rows x 7cols (11,7)\n",
    "#     X_axis_poly = add_features(np.linspace(0,10,100), value) # 1row x 100cols -> 100rows x 7cols (100,7)\n",
    "    X_test_poly = add_features(X_test, value)                # 1row x  4cols  ->   4rows x 7cols (4,7)\n",
    "\n",
    "    # learn\n",
    "    linreg = LinearRegression().fit(X_train_poly, y_train)\n",
    "\n",
    "    # score\n",
    "    print('poly degree ', value, 'score (test): {:.3f}'.format(linreg.score(X_test_poly, y_test)))\n",
    "\n",
    "    # use\n",
    "    #y_train_predict_one = linreg.predict([list(X_train_poly[0,:])]) #input=> 1rows x 7cols, output=> 1.22595734, but y_train[0] = 1.21213026\n",
    "    #y_test_predict_one = linreg.predict([list(X_test_poly[0,:])])   #input=> 1rows x 7cols, output=> 0.98597635, but y_test[0] = 0.99517935\n",
    "    y_test_predict = linreg.predict(X_test_poly)   #input=> 4rows x 7cols, output=> 1row x 4cols: array([ 0.98597635, -0.18539455,  0.37296501,  1.18016858]), but y_test = [ 0.99517935, -0.16081   ,  0.3187423 ,  1.53763897]\n",
    "    #y_axis_predict = linreg.predict(X_axis_poly)   #input=> 100rows x 7cols, output=> 1row x 100cols\n",
    "    print('poly degree ', value, 'predicts: ',y_test_predict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next in Module 2:\n",
    "\n",
    "# Linear models for classification\n",
    "#     Logistic regression\n",
    "#     Logistic regression for binary classification on fruits dataset using height, width features (positive class: apple, negative class: others)\n",
    "#     Logistic regression on simple synthetic dataset\n",
    "#     Logistic regression regularization: C parameter\n",
    "#     Application to real dataset\n",
    "\n",
    "# Support Vector Machines\n",
    "#     Linear Support Vector Machine\n",
    "#     Linear Support Vector Machine: C parameter\n",
    "#     Application to real dataset\n",
    "    \n",
    "# Multi-class classification with linear models\n",
    "#     LinearSVC with M classes generates M one vs rest classifiers.\n",
    "#     Multi-class results on the fruit dataset\n",
    "    \n",
    "# Kernelized Support Vector Machines\n",
    "#     Classification\n",
    "#     Support Vector Machine with RBF kernel: gamma parameter\n",
    "#     Support Vector Machine with RBF kernel: using both C and gamma parameter\n",
    "#     Application of SVMs to a real dataset: unnormalized data\n",
    "#     Application of SVMs to a real dataset: normalized data with feature preprocessing using minmax scaling\n",
    "        \n",
    "# Cross-validation\n",
    "#     Example based on k-NN classifier with fruit dataset (2 features)\n",
    "#     A note on performing cross-validation for more advanced scenarios.\n",
    "#     Validation curve example\n",
    "\n",
    "# Decision Trees\n",
    "#     Setting max decision tree depth to help avoid overfitting\n",
    "#     Visualizing decision trees\n",
    "#     Pre-pruned version (max_depth = 3)\n",
    "#     Feature importance\n",
    "#     Decision Trees on a real-world dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
