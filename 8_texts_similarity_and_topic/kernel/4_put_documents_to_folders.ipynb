{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files in folders= [['plato.txt', 'socrates.txt', 'airplane.txt'], ['electricity.txt'], ['monkey.txt', 'bear.txt']]\n",
      "folders= ['state_use_make', 'make_push_charge', 'state_include_animal']\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "# 1) READ DATA\n",
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "import numpy as np\n",
    "import nltk # nltk.download('popular') # run it once    # nltk.download('punkt')   # run it once (try without it)\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "MINSIM = 0.5 # for test data = 0.1     # minimal similarity between 2 files. If similarity is more - put files in 1 folder.\n",
    "\n",
    "\n",
    "# test data\n",
    "# doc0 = 'This is a function to test document_path_similarity.'\n",
    "# doc1 = 'Use this function to see if your code in doc_to_synsets and similarity_score is correct!'\n",
    "# doc2 = 'According to the Times, one of their key findings was a financial disclosure form from Barrys Senate confirmation proceedings in 1999 to be a federal appellate judge'\n",
    "# doc3 = 'Again, it seemed completely random. One person reported receiving the alert on their T-Mobile Samsung Note while experiencing absolute radio silence from their iPhone X on the same carrier.)'\n",
    "# doc4 = 'They added that the agency is also unsure why some people received the alert before others. Quartz has asked for more detail on why this is the case.'\n",
    "# doc5 = 'The most common monkey species found in animal research are the grivet, the rhesus macaque, and the crab-eating macaque, which are either wild-caught or purpose-bred.'\n",
    "# docs = [ doc0,doc1,doc2,doc3,doc4,doc5]\n",
    "\n",
    "# real data\n",
    "fnames = ['plato.txt','socrates.txt','airplane.txt','electricity.txt','monkey.txt','bear.txt']\n",
    "docs=[]\n",
    "for name in fnames:\n",
    "    with open('../input/'+name, 'r', encoding='utf8') as f: docs.append(f.read())\n",
    "        \n",
    "# with open('../input/plato.txt', 'r', encoding='utf8') as f: doc0 = f.read()\n",
    "# with open('../input/socrates.txt', 'r', encoding='utf8') as f: doc1 = f.read()\n",
    "# with open('../input/airplane.txt', 'r', encoding='utf8') as f: doc2 = f.read()\n",
    "# with open('../input/electricity.txt', 'r', encoding='utf8') as f: doc3 = f.read()\n",
    "# with open('../input/monkey.txt', 'r', encoding='utf8') as f: doc4 = f.read()\n",
    "# with open('../input/bear.txt', 'r', encoding='utf8') as f: doc5 = f.read()\n",
    "\n",
    "\n",
    "def convert_tag(tag):\n",
    "    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'} # Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\n",
    "    try: return tag_dict[tag[0]]\n",
    "    except KeyError: return None\n",
    "    \n",
    "def text_to_synsets_list(doc): # convert string to similar string with changed words (to similar or delete, if there is not similar)\n",
    "    # input: 'This is a function to test document_path_similarity.'\n",
    "    # output: # [Synset('be.v.01'), Synset('angstrom.n.01'), Synset('function.n.01'), Synset('test.v.01')]\n",
    "    words = nltk.word_tokenize(doc)                     # ['This', 'is', 'a', 'function', 'to', 'test', 'document_path_similarity', '.']\n",
    "    words_n_pos = nltk.pos_tag(words)                   # [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('function', 'NN'), ('to', 'TO'), ('test', 'VB'), ('document_path_similarity', 'NN'), ('.', '.')]\n",
    "    poses = [y for x,y in words_n_pos]                  # ['DT', 'VBZ', 'DT', 'NN', 'TO', 'VB', 'NN', '.']\n",
    "    # just renames by first letter\n",
    "    wntag = [convert_tag(x) for x in poses]             # [None, 'v', None, 'n', None, 'v', 'n', None]\n",
    "    #mix words and PoS first letters\n",
    "    ans = list(zip(words,wntag))                       # [('This', None), ('is', 'v'), ('a', None), ('function', 'n'), ('to', None), ('test', 'v'), ('document_path_similarity', 'n'), ('.', None)]\n",
    "    # similar words from WordNet\n",
    "    synsets = [wn.synsets(x,y) for x,y in ans]          # [[], [Synset('be.v.01'),  Synset('exist.v.01'),  Synset('equal.v.01'),...\n",
    "    # remove empty groups and all synsets expect 1 in each groups\n",
    "    final = [val[0] for val in synsets if len(val) > 0] \n",
    "    return final # [Synset('be.v.01'), Synset('angstrom.n.01'), Synset('function.n.01'), Synset('test.v.01')]\n",
    "\n",
    "def compare_2_sysnet_lists(s1, s2):\n",
    "    scores_best =[]\n",
    "    for i in s1:\n",
    "        #print(i, 'first')        # Synset('be.v.01')  /n  Synset('angstrom.n.01')  /n  Synset('function.n.01')  /n  Synset('test.v.01')\n",
    "        scores_all =[]\n",
    "        for j in s2:\n",
    "            # print(j) # 4 loops of: Synset('use.v.01')   Synset('function.n.01')   Synset('see.v.01')   Synset('code.n.01')   Synset('inch.n.01')   Synset('be.v.01')   Synset('correct.a.01')\n",
    "            # if words are similar - path_similarity returns 1 (be and be)\n",
    "            #print(i.path_similarity(j)) # 0.33 0.14 0.25 0.14 0.11 1.0 0.33 None 0.1 None 0.1 0.25 None None None ...\n",
    "            similarity = i.path_similarity(j)\n",
    "            if (similarity != None): scores_all.append(similarity)\n",
    "        if scores_all: scores_best.append(max(scores_all))\n",
    "    # scores_best                     # [1.0, 0.25, 1.0, 0.2]      \n",
    "    return sum(scores_best)/len(scores_best) # 0.6125\n",
    "    \n",
    "\n",
    "def find_topics_of_string(s, topic_count):\n",
    "    import pickle\n",
    "    import gensim #anaconda prompt -> pip install -U gensim\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    data = [s]\n",
    "    vect = CountVectorizer(min_df=0, max_df=1, stop_words='english', token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "    X = vect.fit_transform(data)\n",
    "    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "    id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, id2word=id_map, num_topics=1, passes=25, random_state=34) # wait too long\n",
    "    return ldamodel.print_topics(num_topics=1, num_words=topic_count) #[0][1].split('+')[0].split('*')[1]\n",
    "    \n",
    "def clean_topics(top): #[(0,   '0.031*\"plato\" + 0.011*\"socrates\")] => ['plato', 'socrates']\n",
    "    return [x.split('*')[1].replace('\"', '').rstrip() for x in top[0][1].split('+')]    \n",
    "    \n",
    "def list_to_sentence(doc):\n",
    "    return ''.join(e+' ' for e in doc).rstrip()\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "# 2) GET FOREACH DOC:\n",
    "# - TOPICS => [(0, '0.333*\"document_path_similarity\" + 0.333*\"function\" + 0.333*\"test\"')] \n",
    "# - THEN SYNSETS LIST (GOOD FOR COMPARING) => [Synset('function.n.01'), Synset('trial.n.02')]\n",
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "synsets=[]\n",
    "for doc in docs:\n",
    "#    synsets.append( (text_to_synsets_list(list_to_sentence(clean_topics(find_topics_of_string(doc[0], 1000)))) , doc[1] ) )\n",
    "   synsets.append( text_to_synsets_list(list_to_sentence(clean_topics(find_topics_of_string(doc, 1000)))) )\n",
    "\n",
    "# test print all similarities\n",
    "# for i in range(len(docs)):\n",
    "#     for j in range(len(docs)):\n",
    "#         if not j==i:\n",
    "#             sim = compare_2_sysnet_lists(synsets[i], synsets[j])\n",
    "#             sim = (int(sim*100))/100\n",
    "#             print(sim,fnames[i],fnames[j])\n",
    "# 0.67 plato.txt socrates.txt\n",
    "# 0.5 plato.txt airplane.txt\n",
    "# 0.36 plato.txt electricity.txt\n",
    "# 0.47 plato.txt monkey.txt\n",
    "# 0.48 plato.txt bear.txt\n",
    "# 0.66 socrates.txt plato.txt\n",
    "# 0.48 socrates.txt airplane.txt\n",
    "# 0.37 socrates.txt electricity.txt\n",
    "# 0.42 socrates.txt monkey.txt\n",
    "# 0.45 socrates.txt bear.txt\n",
    "# 0.49 airplane.txt plato.txt\n",
    "# 0.47 airplane.txt socrates.txt\n",
    "# 0.39 airplane.txt electricity.txt\n",
    "# 0.42 airplane.txt monkey.txt\n",
    "# 0.43 airplane.txt bear.txt\n",
    "# 0.5 electricity.txt plato.txt\n",
    "# 0.53 electricity.txt socrates.txt\n",
    "# 0.57 electricity.txt airplane.txt\n",
    "# 0.4 electricity.txt monkey.txt\n",
    "# 0.43 electricity.txt bear.txt\n",
    "# 0.45 monkey.txt plato.txt\n",
    "# 0.42 monkey.txt socrates.txt\n",
    "# 0.43 monkey.txt airplane.txt\n",
    "# 0.29 monkey.txt electricity.txt\n",
    "# 0.5 monkey.txt bear.txt\n",
    "# 0.5 bear.txt plato.txt\n",
    "# 0.49 bear.txt socrates.txt\n",
    "# 0.47 bear.txt airplane.txt\n",
    "# 0.34 bear.txt electricity.txt\n",
    "# 0.54 bear.txt monkey.txt\n",
    "\n",
    "\n",
    "# print(compare_2_sysnet_lists(s1, s2) #0.6789118246687038  plato vs socrates\n",
    "# compare_2_sysnet_lists(doc1, doc3) #0.5099787370815394   plato vs airplane\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "# 3) SEPARATE DOCS TO FOLDERS => [['plato.txt', 'socrates.txt', 'airplane.txt', 'monkey.txt'], ['electricity.txt', 'bear.txt']]\n",
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "folders=[]\n",
    "def in_folders(name):\n",
    "    for folder in folders:\n",
    "        for file in folder:\n",
    "            if file == name: return True\n",
    "    return False\n",
    "\n",
    "def separate_by_folders(synsets, fnames): # [[Synset('function.n.01'), Synset('trial.n.02')], [Synset('code.n.01'), ...], [...]]\n",
    "    for i in range(len(synsets)):\n",
    "        if not in_folders(fnames[i]):\n",
    "            folder = []\n",
    "            folder.append(fnames[i])\n",
    "            for j in range(len(synsets)):\n",
    "                if not in_folders(fnames[j]):\n",
    "                    if not j==i:\n",
    "                        similarity = compare_2_sysnet_lists(synsets[i],synsets[j])\n",
    "                        if similarity>MINSIM:\n",
    "                            #print(fnames[j], i, j)\n",
    "                            folder.append(fnames[j])\n",
    "            folders.append(folder)\n",
    "    return folders # [['plato.txt', 'socrates.txt', 'airplane.txt', 'monkey.txt'], ['electricity.txt', 'bear.txt']]\n",
    "folders = separate_by_folders(synsets, fnames) \n",
    "print('files in folders=',folders)\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "# 3) FIND NAME FOREACH FOLDER BY TOPIC SEARCH.\n",
    "# FOREACH DOC:\n",
    "# INPUT - [ [Synset('absolute.a.01'), Synset('alert.n.01'), ...] , [], [] ]\n",
    "# OUTPUT - \"function_harmonize_people\"\n",
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "def convert_synsets_to_one_sentence(synsets): # [ [Synset('absolute.a.01'), Synset('alert.n.01'), ...] , [], [] ]\n",
    "    import re\n",
    "    result=[]\n",
    "    for line in synsets:    # [Synset('absolute.a.01'), Synset('alert.n.01'), ...] \n",
    "        newline=[]\n",
    "        for word in line:   # Synset('absolute.a.01')\n",
    "            regex = re.compile(r'^Synset\\(\\'(.*?)(\\.).*$', re.IGNORECASE)\n",
    "            newline.append(regex.sub(r'\\1',str(word)))\n",
    "        result.append(list_to_sentence(newline))\n",
    "    return list_to_sentence(result)                         # ['absolute alert ... macaque ...']\n",
    "\n",
    "def get_topics(line, count=3): # \"function trial code ... correct\"\n",
    "    return list_to_sentence(clean_topics(find_topics_of_string(line,count))).replace(' ', '_') # function_harmonize_people\n",
    "   \n",
    "def find_folder_names(folders):    \n",
    "    folder_names = []\n",
    "    for i in range(len(folders)):\n",
    "        folder_synsets = []\n",
    "        # get list of lists of synsets for current doc\n",
    "        for file in folders[i]: folder_synsets.append(synsets[fnames.index(file)]) # result: [ [Synset('absolute.a.01'), Synset('alert.n.01'), ...] , [], [] ]\n",
    "        # list of lists of synsets for this doc => one string sentence\n",
    "        sentence = convert_synsets_to_one_sentence(folder_synsets) # result=str: \"function trial code ... correct\"\n",
    "        folder_name = get_topics(sentence,3)              # result=str: \"function_harmonize_people\"\n",
    "        folder_names.append(folder_name)\n",
    "    return folder_names # ['function_harmonize_people', 'research_receive_person']\n",
    "folder_names = find_folder_names(folders) \n",
    "print('folders=',folder_names)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "# 4) CREATE FOLDERS AND MOVE FILES THERE\n",
    "#--------------------------------------------------------------\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "for i in range(len(folders)):\n",
    "    folder = folder_names[i]\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs('../input/'+folder)\n",
    "        for file in folders[i]: shutil.move('../input/' + file, '../input/' + folder + '/' + file)    \n",
    "    \n",
    "    \n",
    "# for MINSIM 0.5:\n",
    "#     files in folders= [['plato.txt', 'socrates.txt', 'airplane.txt'], ['electricity.txt'], ['monkey.txt', 'bear.txt']]\n",
    "#     folders= ['state_use_make', 'make_push_charge', 'state_include_animal']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
